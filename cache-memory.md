# 캐시 메모리

## 캐시 메모리가 없을 때 어떤 병목이 만들어지는가?
CPU가 메모리에 접근하는 시간은 CPU의 연산 속도보다 압도적으로 느리다.

실제 현대 최신 CPU와 최신 DDR 메모리로 하여금 예시를 들어보겠다.

`AMD 최대 5200클럭의 8코어 16쓰레드` CPU제품이 캐시 메모리가 없다고 가정해보자.

이 시스템에서 사용하는 메모리는 `DDR5 5600Mhz CL40` 이라고 가정해보자.

DDR 메모리는 하나의 클럭 주기에 대해 두 개의 데이터를 보낼 수 있다.

그럼 우선 메모리의 클럭 주기를 계산해보자.

Data Rate는 DDR이므로 5600 / 2 = 2800MHz 이며 이때 1 에서 2800MHz를 나누면 클럭 주기가 된다.

`1 / 2800Mhz = 0.357ns`가 된다.

이때 클럭 주기에 CL값을 곱하면 `CAS 지연시간` 구할 수 있다.

**CAS 지연시간**이란 CPU가 메모리에 특정 열의 데이터를 요청한 후 실제로 데이터가 준비되어 나오기까지 걸리는 시간을 말한다.

`1 % 2800Mhz x 40 = 14.28ns(CAS 지연 시간 - CAS Latency)`라는 계산 결과가 나온다.

앞서 설명한 CAS 지연시간의 개념에서 포함되지 않은 CPU가 메모리에 요청을 하는데 걸리는 시간 및 데이터가 데이터 버스를 통해 CPU까지 가는 시간 등 여러 지연시간이 추가된다.

그렇기에 실제로는 CPU가 메모리에게 데이터 요청시 걸리는 시간은 약 60ns ~ 80ns가 소요된다.

최대 5200 클럭을 지원하는 CPU이기에 이 CPU코어들은 1초에 52억 번 동작할 수 있음을 의미한다.

CPU의 한번 동작에 걸리는 시간은 1 second / 52억 = 0.192ns이다.

CPU는 0.192ns 단위로 명령어를 처리하다가 갑자기 다음 명령어에서 메모리의 데이터를 가져와야함을 깨닫는다.

그러고 메모리에서 데이터를 가져오는데 걸리는 시간이 60~80ns이다.

CPU는 약 312 ~ 417 단위의 사이클동안 쉬며 메모리를 기다려야한다.

> ## 실생활로 비유해보자.
쿠팡에서 열심히 1분에 하나씩 포장을 진행하고 있는 워커가 있다.

포장지는 항상 내 눈앞에 쌓여있어 손을 뻗어 가져와 포장을 진행하는데, 어느 순간 다 떨어져 없는 경우가 존재할 것이다. 그럴 때 포장지를 다시 내 눈앞에 몇천개 쌓아놓고 작업하기 위해

다른 직원이 포장지를 가져오기 위해 포장지 창고에서 포장지를 가져온다. 근데 이 시간이 대략 312 ~ 417분이 걸린다는 것이다.

쿠팡 사장님 입장에서 이 300여분 이상 놀고있는 워커들을 보면 월급이 아까울 것이다. 

___
위 비유처럼 메모리와 CPU의 병목은 생각보다 심각하다. 심지어 위 예시는 캐시가 존재하는 경우를 빗대어 표현한 것이다. 만약 캐시 메모리가 존재하지 않고 하나의 포장을 할 때마다 300여분이 걸려

포장지를 포장지 창고에서 하나씩 가져와서 포장한다면 300여분에 하나씩 제품 하나를 포장을 하는 것이다.

## 저장장치 계층 구조(memory hierarchy)

![image](https://github.com/user-attachments/assets/e055dfd2-ccad-4a7f-8252-7e62af507fb2)

이해하기 쉬운 법칙 두 개를 소개하자면, CPU와 가까운 저장 장치는 빠르고 멀리 있는 저장 장치는 느리다는 것이다.

빠른 저장장치는 저장 용량이 작고 용량 대비 가격이 매우 비싸다.

레지스터나 캐시같은 경우 CPU안에 가격이 포함되어있어서 이 가격구조를 느끼기 어렵지만, 램과 하드디스크를 용량대비 가격으로 따지면 체감이 가능하다.

## 캐시 메모리에 대해 자세히 알아보자.

일반적으로 CPU와 메모리 사이에 위치한, 레지스터보다는 용량이 크고 메모리(DRAM기반)보다는 빠른 SRAM 기반의 저장 장치이다. 라고 배우는 경우가 많다.

이러고 넘어갈 경우 이상한 오개념이 발생할 수 있는데, 그렇다면 메인보드 상 CPU칩과 메모리사이 어딘가에 SRAM이 존재하는건가? -> 그렇다면 내가 따로 SRAM을 구매하지는 않았는데 메인보드를 구매할 때 이것이 포함되는건가? 라는 의문이 생긴다.

첫 줄에서 말한 `CPU와 메모리 사이`에서 CPU는 코어를 뜻한다. 즉 우리가 아는 그 CPU 칩에 SRAM(캐시 메모리)가 박혀있다.

더 자세히는 L1, L2, L3 캐시 메모리로 분류되며 L3 캐시 메모리만이 CPU 코어들 밖에 위치하여 모든 코어가 이를 공유하는 형태이며 L1, L2 캐시 메모리는 각 코어들 주변에 배치되어 레지스터처럼 각각의 코어에 대해 활용된다.

![image](https://github.com/user-attachments/assets/386aa647-49a1-4c56-a8a9-64b6faeeefa1)

캐시 L시리즈 중, L1 캐시가 가장 빠르며 L3 캐시가 가장 느리다. 당연하게도 L1이 가장 저장용량이 적고 L3가 저장용량이 가장 크다.

CPU 코어는 레지스터에 값을 배치하기 위해 L1 -> L2 -> L3 -> 메모리 -> SSD or 하드디스크 와 같은 순으로 데이터를 찾게 되는 것이다.

### L1 캐시의 분할

L1 캐시는 다시한번 명령어만을 다루는 L1D 캐시, 데이터만을 담는 L1I 캐시로 나뉘어진다.

최적화를 위한 것이며 파이프라이닝을 다시 생각해보면 인출 -> 해석 -> 실행에서

한 명령어를 실행하면서 동시에 다른 데이터를 읽어오는 등의 동시 처리가 가능해진다.

또한 명령어는 순차적 접근이 일반적이고, 데이터는 비순차적 접근이 일반적이기에 둘을 분리하는 것은 합리적인 선택이다.

## 참조 지역성의 원리

캐시 메모리가 비어있을 때는 우선 메모리에서 데이터를 가져와야할 것이다.

그렇다면 캐시 메모리는 메모리의 어떤 데이터를 가져와야 할까?

이에 대한 답은 당연하게도 `CPU가 자주 사용할 법한 데이터`를 예측해서 저장해놓아 준다면 가장 이상적일 것이다.

CPU가 데이터를 요청했는데 그 데이터가 캐시에 있었을 경우 이를 `캐시 히트`라고 한다.

CPU가 데이터 요청시 그 데이터가 캐시에 없었을 경우는 `캐시 미스`라고 한다.

백엔드 개발에서 캐시 기술을 이용할 때도 동일한 개념이 사용된다.

`캐시 히트` / `캐시 히트 + 캐시 미스` 를 계산하면 `캐시 적중률`을 구할 수 있고 요즈음의 CPU들은 90%대의 캐시 적중률을 보인다.

`참조 지역성의 원리는` CPU는 최근에 접근했던 메모리 공간에 다시 접근하려는 경향이 있고 최근에 접근했던 공간 근처를 접근하려는 경향이 있다는 원리이다.

우리의 프로그램들의 일반적 흐름에 대해 생각해보면 위의 원리가 이해된다.

보통 반복적인 작업이 많으며, 순차적 탐색이 많다. 그렇기에 동일한 것이나 바로 다음의 것이나 이전의 것과 같은 근처의 것들을 접근하려는 것이다.

현대의 CPU가 높은 캐시 히트율을 보이는 것은 이러한 원리를 이용하여 최대한 최적화한 결과이다.

